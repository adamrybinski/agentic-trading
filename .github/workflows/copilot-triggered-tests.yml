name: Copilot Triggered Tests

# This workflow is triggered only by PR comments
on:
  pull_request_review_comment:
    types: [created]

jobs:
  check-copilot-completion:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request_review_comment'
    
    permissions:
      contents: read
      issues: write
      pull-requests: write
      actions: write
    
    outputs:
      should-run-tests: ${{ steps.check.outputs.should-run }}
      branch-name: ${{ steps.check.outputs.branch }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Check if tests should run
        id: check
        run: |
          echo "Checking PR comment for test trigger..."
          
          # Only check PR review comments
          COMMENT_BODY="${{ github.event.comment.body }}"
          echo "‚úÖ PR comment detected, running tests"
          echo "should-run=true" >> $GITHUB_OUTPUT
          echo "branch=${{ github.event.pull_request.head.ref }}" >> $GITHUB_OUTPUT

  wait-for-copilot:
    needs: check-copilot-completion
    runs-on: ubuntu-latest
    if: needs.check-copilot-completion.outputs.should-run-tests == 'true'
    
    steps:
      - name: Wait for potential additional commits
        run: |
          echo "‚è≥ Waiting 30 seconds for any additional commits..."
          sleep 30

  run-tests:
    needs: [check-copilot-completion, wait-for-copilot]
    runs-on: ubuntu-latest
    if: needs.check-copilot-completion.outputs.should-run-tests == 'true'
    timeout-minutes: 30
    
    permissions:
      contents: read
      issues: write
      pull-requests: write
      actions: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for all branches
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y swi-prolog
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
      
      - name: Run comprehensive tests
        id: test-run
        continue-on-error: true
        run: |
          echo "üß™ Running comprehensive tests..."
          
          # Run workflow tests
          echo "üìã Testing workflow functionality..."
          python test_workflow.py > test_workflow_output.log 2>&1
          WORKFLOW_EXIT=$?
          
          # Run Prolog tests
          echo "üßÆ Testing Prolog generation..."
          python test_prolog_generation.py > test_prolog_gen_output.log 2>&1
          PROLOG_GEN_EXIT=$?
          
          echo "üîç Testing Prolog analysis..."
          python test_prolog_analysis.py > test_prolog_analysis_output.log 2>&1
          PROLOG_ANALYSIS_EXIT=$?
          
          # Test GitHub Models integration
          echo "ü§ñ Testing GitHub Models integration..."
          GITHUB_TOKEN=mock_token_for_testing python -c "
          from github_models_analyzer_enhanced import GitHubModelsAnalyzer
          analyzer = GitHubModelsAnalyzer()
          print('‚úÖ GitHub Models analyzer initialized successfully')
          " > test_github_models_output.log 2>&1
          GITHUB_MODELS_EXIT=$?
          
          # Run failure simulation test
          echo "üß™ Testing failure simulation..."
          python test_failure_simulation.py > test_failure_sim_output.log 2>&1
          FAILURE_SIM_EXIT=$?
          
          # Collect all exit codes
          if [[ $WORKFLOW_EXIT -eq 0 && $PROLOG_GEN_EXIT -eq 0 && $PROLOG_ANALYSIS_EXIT -eq 0 && $GITHUB_MODELS_EXIT -eq 0 && $FAILURE_SIM_EXIT -eq 0 ]]; then
            echo "test-result=success" >> $GITHUB_OUTPUT
            echo "‚úÖ All tests passed!"
          else
            echo "test-result=failure" >> $GITHUB_OUTPUT
            echo "‚ùå Some tests failed"
            echo "Workflow exit: $WORKFLOW_EXIT"
            echo "Prolog gen exit: $PROLOG_GEN_EXIT" 
            echo "Prolog analysis exit: $PROLOG_ANALYSIS_EXIT"
            echo "GitHub models exit: $GITHUB_MODELS_EXIT"
            echo "Failure sim exit: $FAILURE_SIM_EXIT"
            exit 1
          fi
      
      # Simulate failure for testing (as requested)
      - name: Simulate test failure for demonstration
        if: github.event_name == 'push' && contains(github.event.head_commit.message, 'test-failure-simulation')
        run: |
          echo "üé≠ Simulating test failure for demonstration purposes..."
          echo "This is a simulated failure to test the issue creation workflow"
          exit 1
      
      - name: Generate AI-enhanced issue summary on failure
        if: failure()
        id: ai-summary
        run: |
          echo "ü§ñ Generating AI-enhanced failure summary..."
          
          # Collect test outputs
          WORKFLOW_OUTPUT=""
          if [[ -f test_workflow_output.log ]]; then
            WORKFLOW_OUTPUT=$(tail -50 test_workflow_output.log)
          fi
          
          PROLOG_OUTPUT=""
          if [[ -f test_prolog_analysis_output.log ]]; then
            PROLOG_OUTPUT=$(tail -30 test_prolog_analysis_output.log)
          fi
          
          # Create a comprehensive error report
          ERROR_CONTEXT="
          Test Failure Context:
          - Branch: ${{ github.ref_name }}
          - Commit: ${{ github.sha }}
          - Triggered by: ${{ github.actor }}
          - Event: ${{ github.event_name }}
          
          Workflow Output (last 50 lines):
          $WORKFLOW_OUTPUT
          
          Prolog Test Output (last 30 lines):
          $PROLOG_OUTPUT
          "
          
          # Use GitHub Models to enhance the error analysis (if available)
          python3 << 'EOF' > ai_summary_output.txt
          import os
          import json
          import sys
          
          try:
              from github_models_analyzer_enhanced import GitHubModelsAnalyzer
              
              if os.getenv('GITHUB_TOKEN') and os.getenv('GITHUB_TOKEN') != 'mock_token_for_testing':
                  analyzer = GitHubModelsAnalyzer()
                  
                  error_context = """$ERROR_CONTEXT"""
                  
                  prompt = f"""
                  Analyze this CI/CD test failure and provide a concise summary:
                  
                  {error_context}
                  
                  Please provide:
                  1. Root cause analysis (2-3 sentences)
                  2. Recommended fix (bullet points)
                  3. Priority level (High/Medium/Low)
                  4. Estimated effort (Quick fix/Medium/Complex)
                  
                  Keep the response under 200 words and focus on actionable insights.
                  """
                  
                  try:
                      analysis = analyzer.analyze_text(prompt, model="gpt-4o-mini")
                      print(f"AI_ENHANCED_SUMMARY={analysis}")
                  except Exception as e:
                      print(f"AI_FALLBACK_SUMMARY=AI analysis unavailable: {str(e)}")
              else:
                  print("AI_FALLBACK_SUMMARY=No GitHub token available for AI analysis")
                  
          except ImportError:
              print("AI_FALLBACK_SUMMARY=GitHub Models analyzer not available")
          except Exception as e:
              print(f"AI_FALLBACK_SUMMARY=Error during AI analysis: {str(e)}")
          EOF
          
          # Read the AI output
          AI_OUTPUT=$(cat ai_summary_output.txt)
          echo "$AI_OUTPUT" >> $GITHUB_OUTPUT
      
      - name: Comment on PR with failure details
        if: failure()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            // Get AI summary if available
            let aiSummary = "No AI analysis available";
            try {
              const outputFile = fs.readFileSync('ai_summary_output.txt', 'utf8');
              const match = outputFile.match(/AI_.*?SUMMARY=(.+)/);
              if (match) {
                aiSummary = match[1];
              }
            } catch (e) {
              console.log('Could not read AI summary:', e.message);
            }
            
            const commentBody = `
            ## üö® Test Failure Report
            
            ### üìä Failure Summary
            ${aiSummary}
            
            ### üìã Technical Details
            - **Workflow:** ${context.workflow}
            - **Run Number:** ${context.runNumber}
            - **Branch:** ${context.ref}
            - **Commit:** ${context.sha.substring(0, 7)}
            - **Triggered by:** ${context.actor}
            - **Event:** ${context.eventName}
            
            ### üîç Failure Analysis
            - **Job:** ${{ github.job }}
            - **Time:** ${new Date().toISOString()}
            
            ### üéØ Next Steps
            1. üìã Review the workflow logs for detailed error messages
            2. üîß Check test artifacts for debugging information  
            3. üöÄ Fix failing tests and push new commit
            4. ü§ñ This will trigger automatic re-testing
            
            ### üîó Resources
            - [View Full Logs](${context.payload.repository.html_url}/actions/runs/${context.runId})
            - [Compare Changes](${context.payload.repository.html_url}/compare/${context.sha.substring(0, 7)}^...${context.sha.substring(0, 7)})
            
            @copilot Please analyze this test failure and suggest fixes for the failing tests.
            
            ---
            *Auto-generated by CI/CD workflow failure*
            `;
            
            await github.rest.pulls.createReviewComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: ${{ github.event.pull_request.number }},
              commit_id: context.sha,
              body: commentBody,
              path: '.github/workflows/copilot-triggered-tests.yml',
              line: 1
            });
            
            console.log('Added failure comment to PR');

